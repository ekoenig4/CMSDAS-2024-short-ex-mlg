{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine learning tutorial at CMSDAS at CERN, June 2024\n",
    "## Exercise 5: regression using a convolutional neural network\n",
    "\n",
    "Evan Armstrong Koenig, Matthias Komm, Pietro Vischia\n",
    "\n",
    "The core of this tutorial comes from https://github.com/vischia/data_science_school_igfae2024 (Pietro Vischia (pietro.vischia@cern.ch)).\n",
    "\n",
    "The CMSDAS version extends it to consider a convolutional network to regress Higgs quantities, plus some fixes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Â Convolutional neural networks\n",
    "\n",
    "When processing images, it may be convenient to take into account the fact that neighbouring pixels are usually very correlated (e.g. if one pixel is in the center of an azure eye, there is a high chance that neighbouring bins will be also azure).\n",
    "\n",
    "The intuition by Yann LeCun (LeNet) was to use the operation of convolution to summarize the information of neighbouring bins.\n",
    "\n",
    "\n",
    "$$s(t) = \\int x(a) w(t-a)da$$\n",
    "\n",
    "- When discretized, integral becomes a sum\n",
    "  - $x$ input\n",
    "  - $w$ kernel: specifies how far does the averaging goes\n",
    "  - $s$ feature map\n",
    "\n",
    "An illustration:\n",
    "\n",
    "<img src=\"figs/conv.png\" width=\"80%\"/>\n",
    "\n",
    "\n",
    "Furthermore, the topology of the image is taken into account by *parameter sharing*: the idea is that learning the same structure in different places of the image can be done efficiently if the parameters that learns that structure are the same in different portions of the network. This also introduces a further element of locality similar to the concept of *field of vision*\n",
    "An illustration:\n",
    "\n",
    "<img src=\"figs/conv_field.png\" width=\"80%\"/>\n",
    "\n",
    "\n",
    "Further summarization (e.g. to learn the same concept under different transformations, like rotations) can be achieved by pooling:\n",
    "\n",
    "<img src=\"figs/conv_pooling.png\" width=\"80%\"/>\n",
    "\n",
    "All of this results in an architecture that can learn progressively the structural elements of an array with an image-like structure. The animation is from the original convolutional neural network from Yann LeCun (now at META), that recognized handwritten digits with unprecedented quality.\n",
    "\n",
    "<img src=\"figs/asamples.gif\" width=\"80%\"/>\n",
    "<img src=\"figs/conv_pooling.png\" width=\"80%\"/>\n",
    "\n",
    "Acknowledging that a dataset has an image-like structure consists in deciding that the best *mathematical representation* of this object is that of an image, including e.g. concepts like *closeness*.\n",
    "\n",
    "We will use a CNN for regression, that is we will solve the problem of regression by imposing that the data are populating an \"image\" in the phase space of the detector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment and run this if you are running on Colab (remove only the \"#\", keep the \"!\").\n",
    "# You can run it anyway, but it will do nothing if you have already installed all dependencies\n",
    "# (and it will take some time to tell you it is not gonna do anything)\n",
    "\n",
    "\n",
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')\n",
    "#%cd \"/content/drive/MyDrive/\"\n",
    "#! git clone https://github.com/vischia/data_science_school_igfae2024.git\n",
    "#%cd machine_learning_tutorial\n",
    "#!pwd\n",
    "#!ls\n",
    "#!pip install livelossplot shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "matplotlib.rcParams['figure.figsize'] = (8, 6)\n",
    "matplotlib.rcParams['axes.labelsize'] = 14\n",
    "\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import math\n",
    "import socket\n",
    "import json\n",
    "import pickle\n",
    "import gzip\n",
    "import copy\n",
    "import array\n",
    "import numpy as np\n",
    "import numpy.lib.recfunctions as recfunc\n",
    "from tqdm import tqdm\n",
    "\n",
    "#from scipy.optimize import newton\n",
    "#from scipy.stats import norm\n",
    "\n",
    "import uproot\n",
    "\n",
    "import datetime\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import roc_curve, auc, accuracy_score\n",
    "from sklearn.tree import export_graphviz\n",
    "from sklearn.inspection import permutation_importance\n",
    "try:\n",
    "    # See #1137: this allows compatibility for scikit-learn >= 0.24\n",
    "    from sklearn.utils import safe_indexing\n",
    "except ImportError:\n",
    "    from sklearn.utils import _safe_indexing\n",
    "\n",
    "import pandas as pd\n",
    "import torchinfo\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import data\n",
    "\n",
    "We will use simulated events corresponding to three physics processes.\n",
    "\n",
    "- ttH production\n",
    "- ttW production\n",
    "- Drell-Yan production\n",
    "\n",
    "We will select the multilepton final state, which is a challenging final state with a rich structure and nontrivial background separation.\n",
    "\n",
    "<img src=\"figs/2lss.png\" alt=\"ttH multilepton 2lss\" style=\"width:40%;\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_FOLDER = '/cmsuf/data/store/user/ekoenig/cmsdas/2024/short-ex-mlg'\n",
    "sig = uproot.open(os.path.join(INPUT_FOLDER,'signal.root'))['Friends'].arrays(library=\"pd\")\n",
    "bk1 = uproot.open(os.path.join(INPUT_FOLDER,'background_1.root'))['Friends'].arrays(library=\"pd\")\n",
    "bk2 = uproot.open(os.path.join(INPUT_FOLDER,'background_2.root'))['Friends'].arrays(library=\"pd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\")\n",
    "\n",
    "if torch.backends.mps.is_available() and torch.backends.mps.is_built():\n",
    "    device = torch.device(\"mps\")\n",
    "if torch.cuda.is_available() and torch.cuda.device_count()>0:\n",
    "    device = torch.device(\"cuda\")\n",
    "    \n",
    "print (\"Available device: \",device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Networks\n",
    "\n",
    "A convolutional network specializes in learning from spatially structured data, such as images. The key idea is to use convolutional layers to learn local patterns in the data, which are then combined in deeper layers to learn more complex patterns. To be able to uses a convolutional network, we need to represent our data as 2D images.\n",
    "\n",
    "Lets try to accomplish the regression task using a convolutional network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "signal = sig.drop([\"Hreco_Lep2_pt\", \"Hreco_Lep2_eta\", \"Hreco_Lep2_phi\", \"Hreco_Lep2_mass\", \"Hreco_evt_tag\", \"Hreco_HTXS_Higgs_y\"], axis=1 )\n",
    "\n",
    "X = signal.drop([\"Hreco_HTXS_Higgs_pt\"], axis=1)\n",
    "\n",
    "# learn to regress the log of the pt\n",
    "y = signal[[\"Hreco_HTXS_Higgs_pt\"]].apply(np.log) \n",
    "\n",
    "Nevents = 20_000\n",
    "X = X[:Nevents]\n",
    "y = y[:Nevents]\n",
    "\n",
    "X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "print(\"We have\", len(X_train), \"training samples and \", len(X_test), \"testing samples\")\n",
    "\n",
    "# scale the target variable\n",
    "y_scaler = MinMaxScaler()\n",
    "y_train = y_scaler.fit_transform(y_train)\n",
    "y_test = y_scaler.transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list all objects that have eta/phi coordinates\n",
    "objects = [\n",
    "    \"Hreco_Lep0\",\n",
    "    \"Hreco_Lep1\",\n",
    "    # \"Hreco_HadTop\",\n",
    "    \"Hreco_All5_Jets\",\n",
    "    # \"Hreco_More5_Jets\",\n",
    "    \"Hreco_Jets_plus_Lep\",\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can build a 2D image from the eta/phi coordinates of the leptons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def digitize(x, bins):\n",
    "    # digitize x into bins\n",
    "    idx = np.digitize(x, bins) - 1\n",
    "\n",
    "    # clip values outside the bins\n",
    "    idx = np.clip(idx, 0, len(bins)-2)\n",
    "    return idx\n",
    "\n",
    "def build_image(X, Nbins=5, transforms=[]):\n",
    "    \"\"\"\n",
    "    Build a 2d image of energy deposits\n",
    "    \"\"\"\n",
    "\n",
    "    # using the eta and phi coordinates as x and y\n",
    "    for obj in objects:\n",
    "        X[f'{obj}_x'] = X[f'{obj}_eta']\n",
    "        X[f'{obj}_y'] = X[f'{obj}_phi']\n",
    "\n",
    "    # apply any transformations to the coordinates\n",
    "    for transform in transforms:\n",
    "        X = transform(X)\n",
    "\n",
    "    index = np.arange(len(X))\n",
    "    eta_edges = np.linspace(-5, 5, Nbins+1)\n",
    "    phi_edges = np.linspace(-3.2, 3.2, Nbins+1)\n",
    "\n",
    "    # create the image with the energy deposits\n",
    "    img = np.zeros((len(X), 1, Nbins, Nbins))\n",
    "    for obj in objects:\n",
    "        et = np.sqrt(X[f\"{obj}_pt\"]**2 + X[f\"{obj}_mass\"]**2)\n",
    "        eta_bin = digitize(X[f\"{obj}_x\"], eta_edges)\n",
    "        phi_bin = digitize(X[f\"{obj}_y\"], phi_edges)\n",
    "        img[index, 0, eta_bin, phi_bin] += et\n",
    "\n",
    "    # use the log of the energy instead\n",
    "    img[img > 0] = np.log(img[img > 0])\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# special image feature scaler that ignores the empty pixels in our images\n",
    "class ImageMinMaxScaler:\n",
    "    def __init__(self):\n",
    "        self.max = None\n",
    "        self.min = None\n",
    "\n",
    "    def fit(self, img):\n",
    "        self.max = np.array([ F[F != 0].max() for F in img.transpose(1, 0, 2, 3) ])[None, :, None, None]\n",
    "        self.min = np.array([ F[F != 0].min() for F in img.transpose(1, 0, 2, 3) ])[None, :, None, None]\n",
    "\n",
    "    def transform(self, img):\n",
    "        return np.where(img != 0, (img - self.min) / (self.max - self.min), 0)\n",
    "\n",
    "    def fit_transform(self, img):\n",
    "        self.fit(img)\n",
    "        return self.transform(img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = ImageMinMaxScaler()\n",
    "\n",
    "img_train = build_image(X_train, Nbins=5)\n",
    "img_test = build_image(X_test, Nbins=5)\n",
    "\n",
    "\n",
    "img_train = scaler.fit_transform(img_train)\n",
    "img_test = scaler.transform(img_test)\n",
    "\n",
    "print (f\"Image format: {img_train.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.colors as mcolor\n",
    "\n",
    "norm = mcolor.Normalize(vmin=0, vmax=1)\n",
    "\n",
    "def plot_event_image(ax, img, title):\n",
    "    img = np.where(img != 0, img, np.nan)\n",
    "    c = ax.imshow(img[0], norm=norm)\n",
    "    ax.set_title(title)\n",
    "    return c\n",
    "\n",
    "fig, axs = plt.subplots(2, 2, figsize=[12,12])\n",
    "c0 = plot_event_image(axs[0, 0], img_train[0], 'Training Event 0')\n",
    "c1 = plot_event_image(axs[0, 1], img_train[1], 'Training Event 1')\n",
    "c2 = plot_event_image(axs[1, 0], img_train[2], 'Training Event 2')\n",
    "c3 = plot_event_image(axs[1, 1], img_train[3], 'Training Event 3')\n",
    "\n",
    "fig.colorbar(c0, ax=axs)\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, X, y, device=torch.device(\"cpu\")):\n",
    "        self.X = torch.Tensor(X.values if isinstance(X, pd.core.frame.DataFrame) else X).to(device)\n",
    "        self.y = torch.Tensor(y.values if isinstance(X, pd.core.frame.DataFrame) else y).to(device)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        label = self.y[idx]\n",
    "        datum = self.X[idx]\n",
    "        \n",
    "        return datum, label\n",
    "    \n",
    "train_dataset = MyDataset(img_train, y_train, device=device)\n",
    "test_dataset = MyDataset(img_test, y_test, device=device)\n",
    "\n",
    "batch_size=2048\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "batch_x, batch_y = next(iter(train_dataloader))\n",
    "B, C, H, W = batch_x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the `torch.nn.Conv2d` module to perform the convolutional operation. Unlike a dense network, a convolutional network learns a set of filters that are applied to the input images. The filters are learned during training and are used to extract local patterns from the input images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv = nn.Conv2d(in_channels=C, out_channels=32, kernel_size=3, stride=1, device=device) # kernel_size=3 means 3x3 kernel\n",
    "batch_x.shape, conv(batch_x).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the size of the output image is smaller than the input image. This is because the convolution operation is performed by sliding a filter over the input image. The output size is determined by the size of the input image, the size of the filter, and the stride of the convolution operation. \n",
    "\n",
    "Here is an example of a simple convolutional network with two convolutional layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv = nn.Sequential(\n",
    "    nn.Conv2d(in_channels=C, out_channels=32, kernel_size=3, stride=1),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1),\n",
    "    nn.ReLU(),\n",
    "    nn.Flatten(), # flatten the 2D image into a 1D vector\n",
    ").to(device=device)\n",
    "\n",
    "B, F = conv(batch_x).shape\n",
    "print(B, F)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then use dense neural network layers to perform the regression task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear = nn.Sequential(\n",
    "    nn.Linear(F, 32),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(32, 1),\n",
    "    nn.ReLU(),\n",
    ").to(device=device)\n",
    "\n",
    "linear(conv(batch_x)).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Putting this together into a single model, we have the following architecture:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvolutionNet(nn.Module):\n",
    "    def __init__(self, C, H, W):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=C, out_channels=32, kernel_size=3, stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "        )\n",
    "\n",
    "        B, F = self.conv(torch.zeros(1, C, H, W)).shape\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(F, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.linear(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reinitialize our input data for training\n",
    "\n",
    "Nbins = 5\n",
    "\n",
    "transforms = [\n",
    "    # center_objects, \n",
    "    # zoom_objects,\n",
    "]\n",
    "\n",
    "img_train = build_image(X_train, Nbins=Nbins, transforms=transforms)\n",
    "img_test = build_image(X_test, Nbins=Nbins, transforms=transforms)\n",
    "\n",
    "\n",
    "scaler = ImageMinMaxScaler()\n",
    "img_train = scaler.fit_transform(img_train)\n",
    "img_test = scaler.transform(img_test)\n",
    "\n",
    "\n",
    "print (f\"Image format: {img_train.shape}\")\n",
    "fig, axs = plt.subplots(2, 2, figsize=[12,12])\n",
    "c0 = plot_event_image(axs[0, 0], img_train[0], 'Training Event 0')\n",
    "c1 = plot_event_image(axs[0, 1], img_train[1], 'Training Event 1')\n",
    "c2 = plot_event_image(axs[1, 0], img_train[2], 'Training Event 2')\n",
    "c3 = plot_event_image(axs[1, 1], img_train[3], 'Training Event 3')\n",
    "\n",
    "fig.colorbar(c0, ax=axs)\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "\n",
    "train_dataset = MyDataset(img_train, y_train, device=device)\n",
    "test_dataset = MyDataset(img_test, y_test, device=device)\n",
    "\n",
    "batch_size=2048\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "batch_x, batch_y = next(iter(train_dataloader))\n",
    "B, C, H, W = batch_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ConvolutionNet(C, H, W).to(device=device)\n",
    "\n",
    "epochs=50\n",
    "learningRate = 0.1\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learningRate)\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.99)\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "def train_loop(dataloader, model, loss_fn, optimizer, scheduler, device):\n",
    "    size = len(dataloader.dataset)\n",
    "    losses=[] # Track the loss function\n",
    "    # Set the model to training mode - important for batch normalization and dropout layers\n",
    "    # Unnecessary in this situation but added for best practices\n",
    "    model.train()\n",
    "    #for batch, (X, y) in enumerate(dataloader):\n",
    "    for (X,y) in tqdm(dataloader):\n",
    "        # Reset gradients (to avoid their accumulation)\n",
    "        optimizer.zero_grad()\n",
    "        # Compute prediction and loss\n",
    "        pred = model(X)\n",
    "        #if (all_equal3(pred.detach().numpy())):\n",
    "        #    print(\"All equal!\")\n",
    "        loss = loss_fn(pred, y)\n",
    "        losses.append(loss.detach().cpu())\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    scheduler.step()\n",
    "    return np.mean(losses)\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn, device):\n",
    "    losses=[] # Track the loss function\n",
    "    # Set the model to evaluation mode - important for batch normalization and dropout layers\n",
    "    # Unnecessary in this situation but added for best practices\n",
    "    model.eval()\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    # Evaluating the model with torch.no_grad() ensures that no gradients are computed during test mode\n",
    "    # also serves to reduce unnecessary gradient computations and memory usage for tensors with requires_grad=True\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "        #for (X,y) in tqdm(dataloader):\n",
    "            pred = model(X)\n",
    "            loss = loss_fn(pred, y).item()\n",
    "            losses.append(loss)\n",
    "            test_loss += loss\n",
    "            #correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "            \n",
    "    return np.mean(losses)\n",
    "\n",
    "def predict_loop(dataloader, model, device):\n",
    "    predictions=[]\n",
    "    # Set the model to evaluation mode - important for batch normalization and dropout layers\n",
    "    # Unnecessary in this situation but added for best practices\n",
    "    model.eval()\n",
    "    # Evaluating the model with torch.no_grad() ensures that no gradients are computed during test mode\n",
    "    # also serves to reduce unnecessary gradient computations and memory usage for tensors with requires_grad=True\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)\n",
    "            predictions.append(pred)\n",
    "    return torch.cat(predictions).cpu().numpy()\n",
    "\n",
    "\n",
    "train_losses=[]\n",
    "test_losses=[]\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loss=train_loop(train_dataloader, model, loss_fn, optimizer, scheduler, device)\n",
    "    test_loss=test_loop(test_dataloader, model, loss_fn, device)\n",
    "    train_losses.append(train_loss)\n",
    "    test_losses.append(test_loss)\n",
    "    print(\"Avg train loss\", train_loss, \", Avg test loss\", test_loss, \"Current learning rate\", scheduler.get_last_lr())\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(train_losses[1:], label=\"Average training loss\")\n",
    "plt.plot(test_losses[1:], label=\"Average test loss\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "y_pred = predict_loop(test_dataloader, model, device)\n",
    "plt.scatter(y_pred, y_test, marker='o', s=1.)\n",
    "plt.xlabel(\"Predicted value\")\n",
    "plt.ylabel(\"True value\")\n",
    "\n",
    "minim, maxim = np.min(y_test), np.max(y_test)\n",
    "\n",
    "plt.ylim(minim, maxim)\n",
    "plt.xlim(minim, maxim)\n",
    "plt.plot([minim, maxim],[minim, maxim],linestyle='--',c='black')\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "diff = y_pred-y_test\n",
    "hist,_,_ = plt.hist(diff, bins=100)\n",
    "fractions = [2.3,15.85,50,84.15,97.7]\n",
    "percentiles = np.percentile(diff,fractions)\n",
    "for i in range(len(percentiles)):\n",
    "    plt.plot([percentiles[i],percentiles[i]],[0,max(hist)*1.1],c='black',linestyle='--')\n",
    "    plt.text(percentiles[i],max(hist)*1.11,f\"{fractions[i]:.0f}%\")\n",
    "plt.xlabel(\"Predicted value - True value\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ways to improve\n",
    "\n",
    "The model is not very good. Try to improve it by changing the architecture or input data\n",
    "\n",
    "Just like scaling our input features, it is always a good idea to standardize the input images. \n",
    "* Centering the image\n",
    "* Zooming in on the important parts\n",
    "* Rotating the image \n",
    "* Changing the size of our image\n",
    "\n",
    "Here are some examples on how you can do the centering and zooming \n",
    "```\n",
    "def center_objects(X):\n",
    "    mean_x = np.mean([X[f\"{obj}_x\"] for obj in objects])\n",
    "    mean_y = np.mean([X[f\"{obj}_y\"] for obj in objects])\n",
    "\n",
    "    for obj in objects:\n",
    "        X[f\"{obj}_x\"] -= mean_x\n",
    "        X[f\"{obj}_y\"] -= mean_y\n",
    "\n",
    "    return X\n",
    "\n",
    "def zoom_objects(X):\n",
    "    std_x = np.std([X[f\"{obj}_x\"] for obj in objects])\n",
    "    std_y = np.std([X[f\"{obj}_y\"] for obj in objects])\n",
    "\n",
    "    for obj in objects:\n",
    "        X[f\"{obj}_x\"] /= std_x\n",
    "        X[f\"{obj}_y\"] /= std_y\n",
    "\n",
    "    return X\n",
    "```\n",
    "You can add these methods to the code above and try transforming our data\n",
    "\n",
    "These techniques can help the network learn better. We also can always modify the architecture as we did in the DNN\n",
    "* add dropout layers\n",
    "* use a different activation function\n",
    "* add batch normalization layers\n",
    "* reduce the number of layer\n",
    "* reduce the number of nodes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The End"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
